---
title: "Analysis of Personal Google Activity"
author: ""
date: ""
output:
  temisc::html_te:
    toc: false
    fig_width: 8
    fig_height: 8
---

```{r include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

While brainstorming about cool ways to practice text mining with R
I came up with the idea of exploring my own Google search history.
Then, after googling (ironically) if anyone had done something like this, I stumbled upon 
[Lisa Charlotte's blog post](https://lisacharlotterost.github.io/2015/06/20/Searching-through-the-years/). Lisa's post (actually, a series of posts) are from a while back, so
her instructions for how to download your personal Google history and the format
of the downloads (nowadays, it's in a .html file instead of a series of .json files)
are no longer applicable.

I googled a bit more and found a recent
[RPubs write-up by Stephanie Lancz](https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#) that not only included concise instructions on how/where to get personal Google data,
but also how to clean it with `R`!
With the hard work of figuring out how to set up the data provided for me,
I was excited to find out what I could do.

In this write-up (which
can be downloaded from GitHub and re-used for one's own analysis), I explore
different techniques for visualizing and understanding my data.
I do my best to implement methods that are generic and could be applied to any
kind of similar analysis, irregardless of the topic.
Much of my code is guided by the work of others, so I give
provide references for my inspiration where appropriate.

## Analysis

### Setup

To begin, I create a `params` list in order to emulate what one might 
do with a parameterized RMarkdown report (where the `params` would be a 
part of the yaml header.

```{r}
params <-
  list(
    filepath = "data-raw/Tony-My Activity-Search-MyActivity.html",
    name_main = "Tony",
    color_main = "firebrick"
  )
```

Next, following "best practices", I import all of the packages that I'll need.

```{r}
library("dplyr")
library("stringr")
library("xml2")
library("rvest")
library("lubridate")
library("ggplot2")
library("tidytext")
library("stm")
library("temisc") # Personal package.
```

### Import and Clean

Then, on to the "dirty" work of importing and cleaning the data.
I don't deviate much from 
[Stephanie Lancz's methods](https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#)  for extracting data elements
from the .html file

```{r}
# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#

doc_html <- params$filepath
search_archive <- xml2::read_html(doc_html)

# Extract search time.
date_search <-
  search_archive %>%
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>%
  str_extract(pattern = "(?<=<br>)(.*)(?<=PM|AM)") %>%
  mdy_hms()

# Extract search text.
text_search <-
  search_archive %>%
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>%
  str_extract(pattern = '(?<=<a)(.*)(?=</a>)') %>%
  str_extract(pattern = '(?<=\">)(.*)')

# Extract search type.
type_search <-
  search_archive %>% 
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>% 
  str_extract(pattern = "(?<=mdl-typography--body-1\">)(.*)(?=<a)") %>% 
  str_extract(pattern = "(\\w+)(?=\\s)")

# Differences from reference:
# + Using `lubridate::wday()` instead of calling `weekdays()` and coercing to factors.
# + Conver `year` to an integer (from a double).
# + Adding a `time` column to use for a later visualization.
# + Adding a `name` column to make this code more "parametric".
data <-
  tibble(
    name = params$name_main,
    timestamp = date_search,
    date = lubridate::as_date(date_search),
    year = lubridate::year(date_search) %>% as.integer(),
    month = lubridate::month(date_search, label = TRUE),
    wday = lubridate::wday(date_search, label = TRUE),
    hour = lubridate::hour(date_search),
    time = lubridate::hour(timestamp) + (lubridate::minute(timestamp) / 60),
    type = type_search,
    text = text_search
  )

```

Notably, there are some rows that did not get parsed correctly. I decide to exclude
them from the rest of the analysis.

```{r}
data %>% count(year, sort = TRUE)
data <- data %>% filter(!is.na(year))
```

### EDA Visualizations

Next, it's time to start doing some basic exploratory data analysis (EDA).
Given the temporal nature of the data, an easy EDA approach to implement
is visualizaion across different time periods. To save myself some effort
(or, as I like to see it, make my code more efficient), I create a 
visualization "helper" function. Notably, the `geom` to use is a parameter to this function.
Through experimentation, I found that `ggplot2::geom_bar()` and `ggplot2::geom_hist()` seem
to work better with different temporal periods.

```{r}
# Reference:
# + https://juliasilge.com/blog/ten-thousand-data/.
visualize_bytime <-
  function(viz,
           x_char = NULL,
           geom = c("bar", "hist"),
           colors = NULL,
           lab_subtitle = NULL) {
    geom <- match.arg(geom)
    viz_labs <-
      labs(
        x = NULL,
        y = NULL,
        title = "Count Over Time",
        subtitle = lab_subtitle
      )
    viz_theme <-
      teplot::theme_te_b_facet() +
      theme(panel.grid.major.x = element_blank()) +
      # theme(legend.position = "bottom", legend.title = element_blank())
      theme(legend.position = "none")

    if (geom == "bar") {
      viz <-
        viz +
        geom_bar(aes(y = ..count.., fill = name))
    } else if (geom == "hist") {
      viz <-
        viz +
        geom_histogram(aes(y = ..count.., fill = name), bins = 30)
    }
    viz <-
      viz +
      scale_fill_manual(values = colors) +
      facet_wrap(~ name, ncol = 1, strip.position = "right") +
      viz_labs +
      viz_theme
    viz
  }
```

```{r}
lab_subtitle_all <-
  paste0(
    "From ",
    strftime(data$timestamp[1], "%Y-%m-%d"),
    " to ",
    strftime(rev(data$timestamp)[1], "%Y-%m-%d")
  )

viz_bytime_all <-
  data %>%
  ggplot(aes(x = timestamp)) %>%
  add_viz_bytime_elements(geom = "hist", colors = params$color_main, lab_subtitle = lab_subtitle_all)

viz_bytime_yyyy <-
  data %>%
  ggplot(aes(x = year)) %>%
  add_viz_bytime_elements(geom = "bar", colors = params$color_main, lab_subtitle = "By Year")

viz_bytime_mm <-
  data %>%
  ggplot(aes(x = month)) %>%
  add_viz_bytime_elements(geom = "bar", colors = params$color_main, lab_subtitle = "By Month")

viz_bytime_wday <-
  data %>%
  ggplot(aes(x = wday)) %>%
  add_viz_bytime_elements(geom = "bar", colors = params$color_main, lab_subtitle = "By Day of Week")

viz_bytime_hh <-
  data %>%
  ggplot(aes(x = hour)) %>%
  add_viz_bytime_elements(geom = "bar", colors = params$color_main, lab_subtitle = "By Hour of Day")
```

```{r echo = FALSE}
viz_bytime_all
viz_bytime_yyyy 
viz_bytime_mm
viz_bytime_wday
viz_bytime_hh
```

```{r}
viz_bytime_hh_2 <-
  data %>%
  ggplot(aes(x = name, y = time, fill = name)) +
  scale_y_continuous(
    limits = c(1, 24),
    breaks = c(6, 12, 18),
    labels = c("6am", "Noon", "6pm")
  ) +
  scale_fill_manual(values = params$color_main) +
  geom_violin(size = 0, alpha = 0.7) +
  geom_hline(yintercept = seq(3, 24, by = 3),
             color = "gray",
             size = 0.1) +
  labs(x = NULL, y = NULL, title = "Count Over Time", lab_subtitle = "By Time of Day") +
  teplot::theme_te_b_dx() +
  theme(legend.position = "none", panel.grid = element_blank()) +
  coord_flip()

viz_bytime_hh_wday <-
  data %>%
  filter(name == params$name_main) %>% 
  ggplot(aes(x = hour, group = wday, fill = name)) +
  geom_bar() +
  scale_fill_manual(values = params$color_main) +
  facet_wrap( ~ wday, scales = "free") +
  labs(x = NULL, y = NULL, title = "Count Over Time", lab_subtitle = "By Hour of Day and Time of Day") +
  teplot::theme_te_b_facet_dx() +
  theme(legend.position = "none", panel.grid = element_blank())

viz_bytime_wday_yyyy <-
  data %>%
  count(name, year, wday) %>%
  ggplot(aes(x = wday, y = n, fill = year)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, scales = "free") +
  labs(x = NULL, y = NULL, title = "Count Over Time", lab_subtitle = "By Day of Week and Year") +
  # viridis::scale_fill_viridis(discrete = FALSE) +
  teplot::theme_te_b_facet_dx() +
  theme(legend.position = "bottom", panel.grid = element_blank())

```

```{r}
rgx_pattern_search <- '(http|https)\\S+\\s*|(#|@)\\S+\\s*|\\n|\\"|(.*.)\\.com(.*.)\\S+\\s|[^[:alnum:]]'
rgx_replacement_search <- " "

tidy_data_unigrams <-
  function(data = NULL,
           colname_text = "text",
           colname_word = "word",
           rgx_pattern,
           rgx_replacement,
           rgx_unnest,
           stopwords = TRUE,
           stopwords_lexicon,
           rgx_ignore_custom) {
    
    if(is.null(data)) stop("`data` cannot be NULL.", call. = FALSE)
    colname_text_quo <- rlang::sym(colname_text)
    colname_word_quo <- rlang::sym(colname_word)
    
    out <- data
    if (!missing(rgx_pattern) & !missing(rgx_replacement)) {
      out <-
        mutate(out,
               text = str_replace_all(!!colname_text_quo, rgx_pattern, rgx_replacement))
    }
    
    if (missing(rgx_unnest)) {
      out <-
        tidytext::unnest_tokens(out, !!colname_word_quo, !!colname_text_quo)
    } else {
      out <-
        tidytext::unnest_tokens(out, !!colname_word_quo, !!colname_text_quo, rgx_unnest)
    }

    if(stopwords) {
      
      if(missing(stopwords_lexicon)) {
        stop_words <- tidytext::stop_words
      } else {
        stop_words <- tidytext::get_stopwords(source = stopwords_lexicon)
      }
      # out <- dplyr::anti_join(out, stop_words, by = c(colname_word = "word"))
      out <- dplyr::rename(out, word = !!colname_word_quo)
      out <- dplyr::anti_join(out, stop_words, by = "word")
      out <- dplyr::rename(out, !!colname_word_quo := word)
    }
    
    if (!missing(rgx_ignore_custom)) {
      out <-
        filter(out, !str_detect(!!colname_word_quo, rgx_ignore_custom))
    }
    
    out <- filter(out, str_detect(!!colname_word_quo, "[a-z]"))
    out
  }

tidy_data_bigrams <-
  function(data = NULL,
           colname_text = "text",
           colname_words = "bigram", 
           colname_word1 = "first",
           colname_word2 = "second",
           rgx_pattern,
           rgx_replacement,
           stopwords = TRUE,
           stopwords_lexicon,
           rgx_ignore_custom) {

    if(is.null(data)) stop("`data` cannot be NULL.", call. = FALSE)
    colname_text_quo <- rlang::sym(colname_text)
    colname_words_quo <- rlang::sym(colname_words)
    colname_word1_quo <- rlang::sym(colname_word1)
    colname_word2_quo <- rlang::sym(colname_word2)

    out <- data
    if (!missing(rgx_pattern) & !missing(rgx_replacement)) {
      out <-
        dplyr::mutate(out,
               text = stringr::str_replace_all(!!colname_text_quo, rgx_pattern, rgx_replacement))
    }
    out <- tidytext::unnest_tokens(out, !!colname_words_quo, !!colname_text_quo, token = "ngrams", n = 2)
    out <- tidyr::separate(out, !!colname_words_quo, 
                           into = c(colname_word1, colname_word2), sep = " ", remove = FALSE)
    if(stopwords) {
      
      if(missing(stopwords_lexicon)) {
        stop_words <- tidytext::stop_words
      } else {
        stop_words <- tidytext::get_stopwords(source = stopwords_lexicon)
      }
      out <- dplyr::rename(out, word = !!colname_word1_quo)
      out <- dplyr::anti_join(out, stop_words, by = "word")
      out <- dplyr::rename(out, !!colname_word1_quo := word)
      
      out <- dplyr::rename(out, word = !!colname_word2_quo)
      out <- dplyr::anti_join(out, stop_words, by = "word")
      out <- dplyr::rename(out, !!colname_word2_quo := word)
    }
    if (!missing(rgx_ignore_custom)) {
      out <-
        dplyr::filter(out, !stringr::str_detect(!!colname_word1_quo, rgx_ignore_custom))
      out <-
        dplyr::filter(out, !stringr::str_detect(!!colname_word2_quo, rgx_ignore_custom))
    }

    out <- dplyr::filter(out, stringr::str_detect(!!colname_word1_quo, "[a-z]"))
    out <- dplyr::filter(out, stringr::str_detect(!!colname_word2_quo, "[a-z]"))
    out
  }
```

```{r}
data_tidy_unigrams <-
  data %>%
  tidy_data_unigrams(
    rgx_pattern = rgx_pattern_search, 
    rgx_replacement = rgx_replacement_search
  )

data_tidy_unigrams %>% select(word, name) %>% count(word, sort = TRUE)

data_tidy_bigrams <-
  data %>%
  tidy_data_bigrams(
    rgx_pattern = rgx_pattern_search, 
    rgx_replacement = rgx_replacement_search
  )
data_tidy_bigrams %>% select(bigram, name) %>% count(bigram, sort = TRUE)
```

```{r}
get_ngrams_freqs_byx <-
  function(data = NULL, 
           colname_x = "name", 
           colname_cnt = "word") {
  
  if(is.null(data)) stop("`data` cannot be NULL.", call. = FALSE)
  colname_x_quo <- rlang::sym(colname_x)
  colname_cnt_quo <- rlang::sym(colname_cnt)

  ngrams_cnt_1 <- dplyr::count(data, !!colname_x_quo)
  ngrams_cnt_2 <- dplyr::count(data, !!colname_x_quo, !!colname_cnt_quo)

  ngrams_joined <- dplyr::left_join(ngrams_cnt_2, dplyr::rename(ngrams_cnt_1, total = n), by = "name")
  out <- dplyr::mutate(ngrams_joined, freq = n / total)
  out <- dplyr::arrange(out, dplyr::desc(freq))
  out
}

```

```{r}
unigrams_byname_freqs <- get_ngrams_freqs_byx(data_tidy_unigrams, colname_x = "name", colname_cnt = "word")
bigrams_byname_freqs <- get_ngrams_freqs_byx(data_tidy_bigrams, colname_x = "name", colname_cnt = "bigram")
```

```{r}
visualize_ngrams_byname_freqs_wordcloud <-
  function(data, name_filter, color, max_words = 50) {
    data_proc <-
      data %>%
      filter(name == name_filter)
    out <-
      wordcloud::wordcloud(
        word = data_proc$word,
        freq = data_proc$n,
        max.words = max_words,
        random.order = FALSE,
        colors = color
      )
    out
  }
```

```{r, echo = FALSE}
# "3" is a subjective choice.
# num_par_row <- ceiling(length(params$name_main) / 3)
# num_par_col <- min(length(params$name_main), 3)
num_par_row <- 1
num_par_col <- 1
```

```{r, echo = FALSE}
par(mfrow = c(num_par_row, num_par_col))
```

```{r}
purrr::map2(
  params$name_main,
  params$color_main,
  ~visualize_ngrams_byname_freqs_wordcloud(
    data = unigrams_byname_freqs,
    name_filter = .x,
    color = .y
  )
)

```

```{r, echo = FALSE}
par(mfrow = c(1, 1))
```

```{r, echo = FALSE}
par(mfrow = c(num_par_row, num_par_col))
```

```{r, results = "hide"}
purrr::map2(
  params$name_main,
  params$color_main,
  ~visualize_ngrams_byname_freqs_wordcloud(
    data = bigrams_byname_freqs %>% rename(word = bigram),
    name_filter = .x,
    color = .y
  )
)
```


```{r, echo = FALSE}
par(mfrow = c(1, 1))
```

```{r}
# "10" is a subjective choice.
num_top_bigram_freq <- 10
bigrams_byname_freqs_viz <-
  bigrams_byname_freqs %>%
  group_by(name) %>%
  mutate(rank = row_number(desc(freq))) %>%
  filter(rank <= num_top_bigram_freq) %>%
  ungroup() %>%
  arrange(name) %>%
  mutate(bigram = str_replace_all(bigram, " ", "\n")) %>%
  mutate(bigram = forcats::fct_reorder(factor(bigram), freq))

viz_bigrams_byname_freqs <-
  bigrams_byname_freqs_viz %>%
  ggplot(aes(x = name, y = bigram, color = name, size = freq)) +
  geom_point() +
  scale_y_discrete(position = "right") +
  scale_color_manual(values = params$color_main) +
  scale_size_area(max_size = 25) +
  teplot::theme_te_b() +
  theme(legend.position = "none") +
  coord_flip() +
  labs(x = NULL, y = NULL) +
  labs(title = "Most Frequently Used Pairs of Words", subtitle = "By Name")
```

```{r}
viz_bigrams_byname_freqs
```


```{r}
num_top_tfidf <- 10
data_tfidf <-
  data_tidy_unigrams %>%
  count(year, word, sort = TRUE) %>% 
  tidytext::bind_tf_idf(word, year, n) %>% 
  group_by(year) %>%
  # arrange(year, desc(tf_idf)) %>%
  # slice(1:num_top_tfidf) %>%
  top_n(num_top_tfidf, tf_idf) %>% 
  ungroup()

viz_tfidf <-
  data_tfidf %>%
  mutate(year = factor(year)) %>% 
  mutate(word = drlib::reorder_within(word, tf_idf, year)) %>%
  ggplot(aes(word, tf_idf, fill = year)) +
  geom_col() +
  viridis::scale_fill_viridis(discrete = TRUE) +
  facet_wrap(~ year, scales = "free") +
  drlib::scale_x_reordered() +
  coord_flip() +
  teplot::theme_te_b_dx() +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  labs(title = "Highest TF-IDF Words", subtitle = "By Year")
```

```{r}
viz_tfidf
```

```{r}
data_dfm <-
  data_tidy_unigrams %>%
  count(year, word, sort = TRUE) %>%
  tidytext::cast_dfm(year, word, n)

```

```{r eval = FALSE}
model_stm <- stm::stm(data_dfm, K = 6, verbose = FALSE, init.type = "Spectral")
```


```{r include = FALSE}
# saveRDS(model_stm, file.path("data", "model_stm-year-tony.rds"))
model_stm <- readRDS(file.path("data", "model_stm-year-tony.rds"))
```

```{r eval = FALSE, include = FALSE}
data_dfm <-
  data_tidy_unigrams %>%
  count(hour, word, sort = TRUE) %>%
  tidytext::cast_dfm(hour, word, n)
model_stm <- stm::stm(data_dfm, K = 6, verbose = FALSE, init.type = "Spectral")
# saveRDS(model_stm, file.path("data", "model_stm-hour-tony.rds"))
model_stm <- readRDS(file.path("data", "model_stm-hour-tony.rds"))
```

```{r}
model_stm_betas <- broom:::tidy(model_stm)
num_top_beta <- 10
viz_stm_betas <-
  model_stm_betas %>%
  group_by(topic) %>%
  top_n(num_top_beta, beta) %>%
  # arrange(desc(beta)) %>% 
  # slice(1:num_top_beta) %>% 
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = drlib::reorder_within(term, beta, topic)) %>%
  mutate(topic = factor(topic)) %>% 
  ggplot(aes(x = term, y = beta, fill = topic)) +
  geom_col() +
  viridis::scale_fill_viridis(discrete = TRUE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  drlib::scale_x_reordered() +
  teplot::theme_te_b_facet() +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  labs(title = "Highest Word Probabilities for Each Topic")

```

```{r}
viz_stm_betas
```

```{r}
model_stm_gammas <- broom::tidy(model_stm, matrix = "gamma", document_names = rownames(data_dfm))

viz_stm_gammas <-
  model_stm_gammas %>%
  mutate(topic = paste0("Topic ", topic)) %>% 
  mutate(topic = factor(topic)) %>% 
  ggplot(aes(x = gamma, fill = topic)) +
  geom_histogram(bins = 6) +
  viridis::scale_fill_viridis(discrete = TRUE) +
  facet_wrap( ~ topic) +
  teplot::theme_te_b_facet_dx() +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL) +
  labs(title = "Distribution of Document Probabilities for Each Topic")
```

```{r}
viz_stm_gammas
```

