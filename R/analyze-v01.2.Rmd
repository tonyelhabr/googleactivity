---
title: "Analysis of Personal Google Search History"
author: ""
date: ""
output:
  html_document:
    toc: false
    fig_width: 8
    fig_height: 8
---

```{r setup, include = FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```

While brainstorming about cool ways to practice text mining with R
I came up with the idea of exploring my own Google search history.
Then, after googling (ironically) if anyone had done something like this, I stumbled upon 
[Lisa Charlotte's blog post](https://lisacharlotterost.github.io/2015/06/20/Searching-through-the-years/). 
Lisa's post (actually, a series of posts) are from a while back, so
her instructions for how to download your personal Google history and the format
of the downloads (nowadays, it's in a .html file instead of a series of .json files)
are no longer applicable.

I googled a bit more and found a recent
[RPubs write-up by Stephanie Lancz](https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#) 
that not only included concise instructions on how/where to get personal Google data,
but also how to clean it with `R`!
With the hard work of figuring out how to set up the data provided for me,
I was excited to find out what I could do.

In this write-up (which
can be downloaded from GitHub and re-used for one's own analysis), I explore
different techniques for visualizing and understanding my data.
I do my best to implement methods that are generic and could be applied to any
kind of similar analysis, irregardless of the topic.
Much of my code is guided by the work of others, so I give
provide references for my inspiration where appropriate.

## Setup

To begin, I create a `params` list in order to emulate what one might 
do with a parameterized RMarkdown report (where the `params` would be a 
part of the yaml header.

```{r define_params}
params <-
  list(
    filepath = "data-raw/Tony-My Activity-Search-MyActivity.html",
    name_main = "Tony",
    color_main = "firebrick"
  )
```

I'll also go ahead and create a couple of functions for coloring some of the
plots that I'll create. These can be customized to one's personal preferences.

```{r define_scale_x_funcs}
scale_color_func <- function() {
  viridis::scale_color_viridis(
    option = "D",
    discrete = TRUE,
    begin = 0,
    end = 0.75
  ) 
}

scale_fill_func <- function() {
  viridis::scale_fill_viridis(
    option = "D",
    discrete = TRUE,
    begin = 0,
    end = 0.75
  ) 
}
```


Next, following "best practices", I import all of the packages that I'll need.

```{r import_packages}
library("dplyr")
library("stringr")
library("xml2")
library("rvest")
library("lubridate")
library("ggplot2")
library("tidytext")
library("stm")
# devtools::install_github("tonyelhabr/temisc")
library("temisc") # Personal package.
```

### Import and Clean

Then, on to the "dirty" work of importing and cleaning the data.
I don't deviate much from 
[Stephanie Lancz's methods](https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#)
for extracting data elements
from the .html file

```{r ipmort_and_clean_data}
# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#

doc_html <- params$filepath
search_archive <- xml2::read_html(doc_html)

# Extract search time.
date_search <-
  search_archive %>%
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>%
  str_extract(pattern = "(?<=<br>)(.*)(?<=PM|AM)") %>%
  mdy_hms()

# Extract search text.
text_search <-
  search_archive %>%
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>%
  str_extract(pattern = '(?<=<a)(.*)(?=</a>)') %>%
  str_extract(pattern = '(?<=\">)(.*)')

# Extract search type.
type_search <-
  search_archive %>% 
  html_nodes(xpath = '//div[@class="mdl-grid"]/div/div') %>% 
  str_extract(pattern = "(?<=mdl-typography--body-1\">)(.*)(?=<a)") %>% 
  str_extract(pattern = "(\\w+)(?=\\s)")

# Differences from reference:
# + Using `lubridate::wday()` instead of calling `weekdays()` and coercing to factors.
# + Using `yyyy`, `mm`, `wd`, and `hh` instead of `year`, `month`, `wday`, and `hour`.
# + Convert `yyyy` to an integer (from a double).
# + Adding a `time` column to use for a later visualization.
# + Adding a `name` column to make this code more "parametric".
data <-
  tibble(
    name = params$name_main,
    timestamp = date_search,
    date = lubridate::as_date(date_search),
    yyyy = lubridate::year(date_search) %>% as.integer(),
    mm = lubridate::month(date_search, label = TRUE),
    wd = lubridate::wday(date_search, label = TRUE),
    hh = lubridate::hour(date_search),
    time = lubridate::hour(timestamp) + (lubridate::minute(timestamp) / 60),
    type = type_search,
    text = text_search
  )

```

Notably, there are some rows that did not get parsed correctly. I decide to exclude
them from the rest of the analysis.

```{r filter_data_for_na}
data %>% count(yyyy, sort = TRUE)
data <- data %>% filter(!is.na(yyyy))
```

## Analysis

### Search Count Distributions

Next, it's time to start doing some basic exploratory data analysis (EDA).
Given the temporal nature of the data, an easy EDA approach to implement
is visualization across different time periods. To save myself some effort
(or, as I like to see it, make my code more efficient), I create a 
visualization "helper" function. Notably, the `geom` to use is a parameter to this function.
Through experimentation, I found that `ggplot2::geom_bar()` and `ggplot2::geom_hist()` seem
to work better with different temporal periods.

```{r visualize_time_func}
# Reference:
# + https://juliasilge.com/blog/ten-thousand-data/.
visualize_time <-
  function(data,
           colname_x,
           geom = c("bar", "hist"),
           color = "grey50",
           lab_subtitle = NULL) {

    geom <- match.arg(geom)
    viz_labs <-
      labs(
        x = NULL,
        y = NULL,
        title = "Count Over Time",
        subtitle = lab_subtitle
      )
    viz_theme <-
      temisc::theme_te_b_facet() +
      theme(panel.grid.major.x = element_blank()) +
      theme(legend.position = "none")

    viz <- ggplot(data, aes_string(x = colname_x)) 
    if (geom == "bar") {
      viz <-
        viz +
        geom_bar(aes_string(y = "..count.."), fill = color)
    } else if (geom == "hist") {
      viz <-
        viz +
        geom_histogram(aes_string(y = "..count.."), fill = color, bins = 30)
    }

    viz <-
      viz +
      viz_labs +
      viz_theme
    viz
  }
```

Using this function is fairly straightforward. For example, to visualize my count
count of searches by year, it can be invoked in the following manner.

```{r visualize_time_example, eval = FALSE}
viz_time_yyyy <-
  visualize_time(
    data = data,
    colname_x = "yyyy",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Year"
  )
```

The same pattern can be repeated for `timestamp`, `yyyy`, `mm`, `wd`, and `hh`.

```{r visualize_time_usage, include = FALSE}
lab_subtitle_all <-
  paste0(
    "From ",
    strftime(data$timestamp[1], "%Y-%m-%d"),
    " to ",
    strftime(rev(data$timestamp)[1], "%Y-%m-%d")
  )

viz_time_all <-
  visualize_time(
    data = data,
    colname_x = "timestamp",
    geom = "hist",
    color = params$color_main,
    lab_subtitle = lab_subtitle_all
  )

# NOTE: Could probably use `purrr::pmap()` for the following.
viz_time_yyyy <-
  visualize_time(
    data = data,
    colname_x = "yyyy",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Year"
  )

viz_time_mm <-
  visualize_time(
    data = data,
    colname_x = "mm",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Month"
  )

viz_time_wd <-
  visualize_time(
    data = data,
    colname_x = "wd",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Day of Week"
  )

viz_time_hh <-
  visualize_time(
    data = data,
    colname_x = "hh",
    geom = "bar",
    color = params$color_main,
    lab_subtitle = "By Hour"
  )
```

```{r visualize_time_show, echo = FALSE}
viz_time_all
viz_time_yyyy 
viz_time_mm
viz_time_wd
viz_time_hh
```

Of course, other `geom`s can be used to visualize the data in other interesting ways.
For instance, I could create a violin plot for the by-hour counts.

```{r viz_time_unused, include = FALSE, eval = FALSE}
# Reference:
# + https://buzzfeednews.github.io/2018-01-trump-twitter-wars/
viz_time_hh_2 <-
  data %>%
  ggplot(aes(x = name, y = time, fill = name)) +
  scale_y_continuous(
    limits = c(1, 24),
    breaks = c(6, 12, 18),
    labels = c("6am", "Noon", "6pm")
  ) +
  scale_fill_manual(values = params$color_main) +
  geom_violin(size = 0) +
  geom_hline(
    yintercept = seq(3, 24, by = 3),
    color = "gray",
    size = 0.1
  ) +
  labs(x = NULL, y = NULL) +
  labs(title = "Count Over Time", lab_subtitle = "By Time of Day") +
  temisc::theme_te_b_dx() +
  theme(legend.position = "none", panel.grid = element_blank()) +
  coord_flip()

# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
viz_time_hh_wd <-
  data %>%
  ggplot(aes(x = hh, group = wd, fill = name)) +
  geom_bar() +
  scale_fill_manual(values = params$color_main) +
  facet_wrap( ~ wd, scales = "free") +
  labs(x = NULL, y = NULL) +
  labs(title = "Count Over Time", lab_subtitle = "By Hour of Day and Time of Day") +
  temisc::theme_te_b_facet_dx() +
  theme(legend.position = "none", panel.grid = element_blank())

# Reference:
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
viz_time_wd_yyyy <-
  data %>%
  count(name, yyyy, wd) %>%
  ggplot(aes(x = wd, y = n, fill = yyyy)) +
  geom_bar(stat = "identity") +
  facet_wrap(~name, scales = "free") +
  labs(x = NULL, y = NULL) +
  labs(title = "Count Over Time", lab_subtitle = "By Day of Week and Year") +
  temisc::theme_te_b_facet_dx() +
  theme(legend.position = "bottom", panel.grid = element_blank())

```

### Word Frequencies

Now I can "tokenize" the search text into n-grams. I'll parse each
search query into unigrams and bigrams.

```{r ngrams}
# Reference (for regular expression):
# + https://rstudio-pubs-static.s3.amazonaws.com/355045_90b7464be9b4437393670340ad67c310.html#
rgx_patt <- '(http|https)\\S+\\s*|(#|@)\\S+\\s*|\\n|\\"|(.*.)\\.com(.*.)\\S+\\s|[^[:alnum:]]'
rgx_repl <- " "

# References:
# + https://www.tidytextmining.com/
# + https://www.tidytextmining.com/ngrams.html
# + https://www.tidytextmining.com/twitter.html
stop_words <- tidytext::stop_words
unigrams <-
  data %>%
  mutate(text = str_replace_all(text, rgx_patt, rgx_repl)) %>% 
  tidytext::unnest_tokens(word, text) %>% 
  anti_join(stop_words, by = "word") %>% 
  filter(str_detect(word, "[a-z]"))
unigrams %>% select(word) %>% count(word, sort = TRUE)

# References:
# + https://www.tidytextmining.com/
# + https://www.tidytextmining.com/ngrams.html
bigrams <-
  data %>%
  mutate(text = str_replace_all(text, rgx_patt, rgx_repl)) %>% 
  tidytext::unnest_tokens(word, text, token = "ngrams", n = 2) %>% 
  tidyr::separate(word, into = c("word1", "word2"), sep = " ", remove = FALSE) %>% 
  anti_join(stop_words, by = c("word1" = "word")) %>% 
  anti_join(stop_words, by = c("word2" = "word")) %>% 
  filter(str_detect(word1, "[a-z]")) %>% 
  filter(str_detect(word2, "[a-z]"))
bigrams %>% select(word) %>% count(word, sort = TRUE)
```

With the data parsed into tokens, we can visualize counts of individual tokens.

```{r viz_ngram_cnts}
# Reference:
# + https://github.com/dgrtwo/dgrtwo.github.com/blob/master/_R/2016-08-09-trump-data.Rmd.
visualize_cnts <- function(data, color = "grey50", num_top = 20) {
  data %>% 
    count(word, sort = TRUE) %>%
    filter(row_number(desc(n)) <= num_top) %>%
    mutate(word = reorder(word, n)) %>% 
    ggplot(aes(x = word, y = n)) +
    ggalt::geom_lollipop(size = 2, point.size = 4, color = color) +
    coord_flip() +
    temisc::theme_te_b() +
    labs(x = NULL, y = NULL) +
    labs(title = "Count of Words") +
    theme(legend.position = "none") +
    theme(panel.grid.major.y = element_blank())
}

num_top_cnt <- 20
viz_unigram_cnts <-
  visualize_cnts(
    data = unigrams,
    color = params$color_main,
    num_top = num_top_cnt
  )
viz_unigram_cnts

viz_bigram_cnts <-
  visualize_cnts(
    data = bigrams,
    color = params$color_main,
    num_top = num_top_cnt
  )
viz_bigram_cnts
```

Another method of visualizing frequencies is a word cloud.
Normally, I'm staunchly opposed to word clouds; however,
when used to initialize a mental model of the data, they're not so bad. 
I write a basic function so I can use it twice. (Hoorah for functions!)

```{r visualize_cnts_wordcloud}
visualize_cnts_wordcloud <-
  function(data, color, num_top = 50) {
    data_proc <- data %>% count(word, sort = TRUE)
    wordcloud::wordcloud(
      word = data_proc$word,
      freq = data_proc$n,
      random.order = FALSE,
      colors = color,
      max.words = num_top
    )
  }

num_top_cnt_wordcloud <- 50
visualize_cnts_wordcloud(
  data = unigrams,
  color = params$color_main,
  num_top = num_top_cnt_wordcloud
)

visualize_cnts_wordcloud(
  data = bigrams,
  color = params$color_main,
  num_top = num_top_cnt_wordcloud
)

```

Now I compute token frequencies. This function can
be used to compute frequencies for any time of n-gram, as long as the n-grams are
"united" in a single column. (This might not be the case when working with bigrams, trigrams, etc.)

```{r compute_freqs}
compute_freqs <- function(data, colname_cnt = "word") {
  colname_cnt_quo <- rlang::sym(colname_cnt)
  
  data %>% 
    group_by(!!colname_cnt_quo) %>% 
    mutate(n = n()) %>%
    # ungroup() %>% 
    # group_by(!!colname_cnt_quo) %>% 
    summarize(freq = sum(n) / n()) %>% 
    ungroup() %>% 
    arrange(desc(freq))
}

unigrams_freqs <-
  compute_freqs(
    data = unigrams,
    colname_cnt = "word"
  )
unigrams_freqs

bigrams_freqs <-
  compute_freqs(
    data = bigrams,
    colname_cnt = "word"
  )
bigrams_freqs
```

While the frequencies numbers are interesting,
it's almost always easier to contextualize
values with a good visualization.


```{r viz_bigrams_freqs_unused, eval = FALSE, include = FALSE}
# Reference:
# + https://buzzfeednews.github.io/2018-01-trump-twitter-wars/
num_top_bigram_freq <- 10
bigrams_freqs_viz <-
  bigrams_freqs %>%
  group_by(word) %>%
  mutate(rank = row_number(desc(freq))) %>%
  filter(rank <= num_top_bigram_freq) %>%
  ungroup() %>%
  arrange(name) %>%
  mutate(word = str_replace_all(word, " ", "\n")) %>%
  mutate(word = forcats::fct_reorder(factor(word), freq))

viz_bigrams_freqs <-
  bigrams_freqs_viz %>%
  ggplot(aes(x = name, y = word, color = name, size = freq)) +
  geom_point() +
  scale_y_discrete(position = "right") +
  scale_color_manual(values = params$color_main) +
  scale_size_area(max_size = 25) +
  temisc::theme_te_b() +
  theme(legend.position = "none") +
  coord_flip() +
  labs(x = NULL, y = NULL) +
  labs(title = "Most Frequently Used Pairs of Words", subtitle = "By Name")
viz_bigrams_freqs
```

### Word Correlations

Let's add a layer of complexity to our analysis. We can look at correlations
among individual words in each search. I create a fairly robust function here
because I'll need to perform the same actions twice--once just to see the computed
values, and another time to put the data in the proper format for a network visualization.
(We need both the counts and the correlations of each word pair.)

```{r compute_corrs_func}
# Reference:
# + https://www.tidytextmining.com/ngrams.html
# + http://varianceexplained.org/r/seven-fav-packages/.
compute_corrs <-
  function(data = NULL,
           colname_word = NULL,
           colname_feature = NULL,
           num_top_ngrams = 50,
           num_top_corrs = 50,
           return_corrs = TRUE,
           return_words = FALSE,
           return_both = FALSE) {

    colname_word_quo <- rlang::sym(colname_word)
    colname_feature_quo <- rlang::sym(colname_feature)
    
    data_cnt <-
      data %>%
      count(!!colname_word_quo, sort = TRUE)

    data_cnt_top <-
      data_cnt %>% 
      mutate(rank = row_number(desc(n))) %>% 
      filter(rank <= num_top_ngrams)
    
    data_joined <-
      data %>% 
      semi_join(data_cnt_top, by = colname_word) %>% 
      rename(
        word = !!colname_word_quo,
        feature = !!colname_feature_quo
      )
    data_corrs <-
      widyr::pairwise_cor(
        data_joined,
        word,
        feature,
        sort = TRUE,
        upper = FALSE
      )
    
    data_corrs_top <-
      data_corrs %>% 
      mutate(rank = row_number(desc(correlation))) %>% 
      filter(rank <= num_top_corrs)
    
    if(return_both | (return_words & return_corrs)) {
      out <- list(words = data_cnt_top, corrs = data_corrs_top)
    } else if (return_corrs) {
      out <- data_corrs_top
    } else if (return_words) {
      out <- data_cnt_top
    }
    out
  }

num_top_ngrams <- 50
num_top_corrs <- 50

unigrams_corrs <-
  compute_corrs(
    unigrams,
    num_top_ngrams = num_top_ngrams,
    num_top_corrs = num_top_corrs,
    colname_word = "word",
    colname_feature = "timestamp"
  )
unigrams_corrs
```

A network is really useful for visualizing correlation data like this.

```{r viz_corrs_network}
# Reference:
# + http://varianceexplained.org/r/seven-fav-packages/.
unigrams_corrs_list <-
  compute_corrs(
    unigrams,
    num_top_ngrams = num_top_ngrams,
    num_top_corrs = num_top_corrs,
    colname_word = "word",
    colname_feature = "timestamp",
    return_both = TRUE
  )

seed <- 42
set.seed(seed)
viz_corrs_network <-
  igraph::graph_from_data_frame(
    d = unigrams_corrs_list$corrs, 
    vertices = unigrams_corrs_list$words, 
    directed = TRUE
  ) %>% 
  ggraph::ggraph(layout = "fr") +
  ggraph::geom_edge_link(edge_width = 1) +
  ggraph::geom_node_point(aes(size = n), fill = "grey50", shape = 21) +
  ggraph::geom_node_text(ggplot2::aes_string(label = "name"), repel = TRUE) +
  theme_void() +
  theme(legend.position = "none") +
  labs(title = "Network of Pairwise Correlations", subtitle = "By Search")
viz_corrs_network
```

### Word Changes Over Time

I'm interested to see which words I have either used more or less as time has passed.
Determining the highest changes in word usage is not quite as straightforward
as some of the other components of the text analysis so far.
There are various valid approaches that could be implemented. I'll
to implement a fairly simple and comprehensible one. (Specifically,
I'll follow the approach shown in the
[Twitter chapter in the _Tidy Text Mining_ book](https://www.tidytextmining.com/twitter.html#changes-in-word-use).

First, I'll group my word usage by year and look only at the most used words.

```{r unigrams_bytime}
# Reference:
# + https://www.tidytextmining.com/twitter.html#changes-in-word-use.
timefloor <- "year"
top_pct_words <- 0.05
unigrams_bytime <-
  unigrams %>%
  mutate(time_floor = floor_date(timestamp, unit = timefloor)) %>%
  group_by(time_floor, word) %>% 
  summarise(n = n()) %>% 
  ungroup() %>%
  group_by(time_floor) %>%
  mutate(time_total = sum(n)) %>%
  ungroup() %>% 
  group_by(word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  filter(word_total >= quantile(word_total, 1 - top_pct_words)) %>% 
  arrange(desc(word_total))
unigrams_bytime
```

Next, I'll create logistic models for each word-year pair. These models attempt
essentially answer the question "How likely is it that a given word appears in a
given year?"

```{r unigrams_bytime_models}
unigrams_bytime_models <- 
  unigrams_bytime %>% 
  tidyr::nest(-word) %>% 
  mutate(
    models =
      purrr::map(data, ~ glm(cbind(n, time_total) ~ time_floor, ., family = "binomial"))
  )
unigrams_bytime_models

unigrams_bytime_models_slopes <-
  unigrams_bytime_models %>%
  tidyr::unnest(purrr::map(models, broom::tidy)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted_p_value = p.adjust(p.value))
unigrams_bytime_models_slopes
```

The p.values of the logistic models indicate whether or not the change in usage
of a given word over time is non-trivial. I'll look at the words with the smallest
p.values, indicating that they have the most significant change in usage.

```{r viz_unigrams_change_bytime}
num_top_change <- 5
unigrams_bytime_models_slopes_top <-
  unigrams_bytime_models_slopes %>% 
  top_n(num_top_change, -adjusted_p_value) 

viz_unigrams_change_bytime <-
  unigrams_bytime %>% 
  inner_join(unigrams_bytime_models_slopes_top, by = c("word")) %>%
  mutate(pct = n / time_total) %>%
  mutate(label = if_else(time_floor == max(time_floor), word, NA_character_)) %>%
  ggplot(aes(x = time_floor, y = pct, color = word)) +
  geom_line(size = 1.5) +
  ggrepel::geom_label_repel(aes(label = label), nudge_x = 1, na.rm = TRUE) +
  # geom_point() +
  # geom_smooth(method = "loess", se = FALSE) +
  scale_y_continuous(labels = scales::percent_format()) +
  scale_color_func() +
  temisc::theme_te_b_facet() +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL) +
  labs(title = "Largest Changes in Word Frequency")
viz_unigrams_change_bytime
```

### Unique Words

#### Term-Frequency Inverse Document Frequency (TF-IDF) 

Another good way of evaluating my search behavior is to look at term-frequency
inverse-document-frequency (TF-IDF). I'll leave the reader to read the details
in the [_Tidy Text Mining_](https://www.tidytextmining.com/tfidf.html) book, 
but, in a nutshell, TF-IDF provides a good measure
of the most "unique" words in a given document compared to other documents.
For my analysis,
I'll treat the years of search history as document.

```{r viz_tfidf}
# References:
# + https://www.tidytextmining.com/tfidf.html
# + https://juliasilge.com/blog/sherlock-holmes-stm/
data_tfidf <-
  unigrams %>%
  count(yyyy, word, sort = TRUE) %>% 
  tidytext::bind_tf_idf(word, yyyy, n)
data_tfidf

num_top_tfidf <- 10
viz_tfidf <-
  data_tfidf %>%
  group_by(yyyy) %>%
  # arrange(yyyy, desc(tf_idf)) %>%
  # slice(1:num_top_tfidf) %>%
  top_n(num_top_tfidf, tf_idf) %>% 
  ungroup() %>% 
  mutate(yyyy = factor(yyyy)) %>% 
  mutate(word = drlib::reorder_within(word, tf_idf, yyyy)) %>%
  ggplot(aes(word, tf_idf, fill = yyyy)) +
  geom_col() +
  scale_fill_func() +
  facet_wrap(~ yyyy, scales = "free") +
  drlib::scale_x_reordered() +
  coord_flip() +
  temisc::theme_te_b_dx() +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  labs(title = "Highest TF-IDF Words", subtitle = "By Year")
viz_tfidf
```

### Topic Modeling

#### Latent Dirichlet Allocation (LDA)

Now, I'll try my hand at topic modeling, using the principles laid out in
[_Tidy Text Mining_](https://www.tidytextmining.com/topicmodeling.html).
Notably, the book demonstrates the use of the `topicmodels` package
for Latent Dirichlet allocation (LDA), but other
techniques/packages are certainly valid. For example, Julia Silge
provides examples of topic modeling using the `stm` package in a
[a recent blog post exploring _Sherlock Holmes_ text](https://juliasilge.com/blog/sherlock-holmes-stm/).

```{r data_dtm}
# References: (for all of the topic modelling)
# + (https://www.tidytextmining.com/topicmodeling.html)
# + https://juliasilge.com/blog/sherlock-holmes-stm/

data_dtm <-
  unigrams %>%
  count(yyyy, word, sort = TRUE) %>%
  tidytext::cast_dtm(yyyy, word, n)
```

```{r data_dfm, eval = FALSE, include = FALSE}
data_dfm <-
  unigrams %>%
  count(yyyy, word, sort = TRUE) %>%
  tidytext::cast_dfm(yyyy, word, n)
```

After casting to a`dtm` object (a `DocumentTermMatrix`),
I can use the `topicmodels::LDA()` function for topic modeling.
(Warning: This may take a minute or so.) I'm choosing 9 topics because I have
data for 9 separate years. (Admittedly, this is a somewhat naive approach. Given that
the start and end of my data does not correspond with the beginning and ends of years,
perhaps I could slice the data into 12 month intervals. Another approach worth exploring
would compile it into groups of years corresponding to "phases" in my
schooling and career.)

```{r model_lda, eval = FALSE}
lda_k <- 9
seed <- 42
model_lda <- topicmodels::LDA(data_dtm, k = lda_k, control = list(seed = seed))
```

```{r model_lda_stm, eval = FALSE, include = FALSE}
lda_k <- 9
model_lda <- stm::stm(data_dfm, K = lda_k, verbose = FALSE, init.type = "Spectral")
```

```{r model_lda_io, echo = FALSE}
# saveRDS(model_lda, file.path("data", "model_stm-k9-yyyy-tony.rds"))
# model_lda <- readRDS(file.path("data", "model_stm-k9-yyyy-tony.rds"))
# saveRDS(model_lda, file.path("data", "model_lda-k9-yyyy-tony.rds"))
model_lda <- readRDS(file.path("data", "model_lda-k9-yyyy-tony.rds"))
```

#### Word-To-Topic Probabilities

Now we can look at the probability of a given word being associated with a given 
topics (i.e. beta probability).
Admittedly, the results may seem a bit simply because the topics
themselves are not "assigned" a word--rather, they are known simply as topic 1, topic 2, etc.
Nonetheless, looking at which words are most closely associated with each given topic
allows us to infer what the topic is about.

```{r viz_lda_betas}
model_lda_betas <- broom::tidy(model_lda)
num_top_beta <- 10
viz_lda_betas <-
  model_lda_betas %>%
  group_by(topic) %>%
  top_n(num_top_beta, beta) %>%
  # arrange(desc(beta)) %>% 
  # slice(1:num_top_beta) %>% 
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = drlib::reorder_within(term, beta, topic)) %>%
  mutate(topic = factor(topic)) %>% 
  ggplot(aes(x = term, y = beta, fill = topic)) +
  geom_col() +
  scale_fill_func() +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  drlib::scale_x_reordered() +
  temisc::theme_te_b_facet() +
  theme(legend.position = "none") +
  theme(axis.text.x = element_blank()) +
  labs(x = NULL, y = NULL) +
  labs(title = "Highest Word Probabilities for Each Topic")
viz_lda_betas
```

#### Document-To-Topic Probabilities

We can use document-to-topic probabilities (gamma probabilities)
to see how well the model predicts the document (in this case, the year) from which a 
topic comes. I interpret this as like an extension of the beta probabilities--we go
from words to topic with the beta values, then from topic to document with the gamma values.

```{r model_lda_gammas}
model_lda_gammas <-
  broom::tidy(
    # document_names = rownames(data_dtm),
    model_lda, 
    matrix = "gamma",
  )
```

With my data, I found that the document-to-topic assignment would not be one-to-one
if I simply determined that the consensus document choice of the LDA model is
the document with the highest gamma value.
(Put another way, at least on document would not be the consensus choice for any
topic, and visa versa.)
This "issue" would prevent some of the following visualizations from turning out
how I would like them if no action is taken to address it.
(In particular, it makes a confusion matrix "misaligned".)

```{r model_lda_gammas_debug}
# NOTE: The document-to-topic assignment may not be one-to-one!
model_lda_gammas_ranked <-
  model_lda_gammas %>% 
  group_by(topic) %>%
  mutate(rank = row_number(desc(gamma))) %>% 
  arrange(topic, rank) %>% 
  ungroup()
model_lda_gammas_ranked %>% filter(rank == 1)
model_lda_gammas_ranked %>% filter(rank == 2)

model_lda_gammas_ranked %>% 
  filter(rank <= 2) %>% 
  select(-document) %>% 
  tidyr::spread(rank, gamma) %>% 
  mutate(gamma_diff = `1` -`2`) %>% 
  arrange(gamma_diff)
```

Nonetheless, to prepare to visualize the gammas values, I'll simply make naive
assignment of a consensus document for a given topic
by selecting the document with the highest gamma value for each topic.

```{r model_lda_gammas_preds}
model_lda_gammas_preds <-
  model_lda_gammas %>% 
  group_by(topic) %>%
  top_n(1, gamma) %>%
  mutate(rank = row_number(desc(gamma))) %>% 
  ungroup() %>%
  transmute(consensus = document, topic, gamma)
```

Keeping in mind the document that is not assigned to any topic (and the topics
that share the same document assignment), I "manually" fix the assignments
by making a reasonable re-assignment based on the gamma values of the missing
document and its relative value among the documents in the topics that share
a consensus choice.

Note that not everyone may have the same issue with their own data.
And, even if faced with a similar circumstance (or a similar one where multiple
documents do not have obvious assignments), one may choose not to do anything at all
to address the "issue".

```{r model_lda_gammas_preds_fix}
# NOTE: Manual fix for this data set!
topic_fix <- 6
doc_sub <- "2010"
gamma_replcament <-
  model_lda_gammas %>% 
  filter(document == doc_sub, topic == topic_fix ) %>% 
  pull(gamma)

model_lda_gammas_preds <-
  model_lda_gammas_preds %>% 
  mutate(consensus = ifelse(topic == topic_fix , doc_sub, consensus)) %>% 
  mutate(gamma = ifelse(topic == topic_fix, gamma_replcament, gamma))
```

After the fix, I can visualize the gamma values of the document-to-topic
assignments that the model has made (including any "manual" assignments).
Notably, my one "fix" stands out as having a relatively low probability.

```{r viz_lda_gammas_preds}
viz_lda_gammas_preds <-
  model_lda_gammas_preds %>%
  mutate(topic = paste0("Topic ", topic)) %>% 
  mutate(topic = factor(topic)) %>% 
  ggplot(aes(x = topic, y = gamma, color = topic)) +
  # geom_point() +
  ggalt::geom_lollipop(aes(x = topic, y = gamma), size = 2, point.size = 4) +
  scale_color_func() +
  ylim(0, 1) +
  temisc::theme_te_b() +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL) +
  labs(title = "Document-to-Topic Probabilities")
viz_lda_gammas_preds
```

```{r viz_lda_gammas, include = FALSE, eval = FALSE}
# Reference:
# + https://www.tidytextmining.com/nasa.html#topic-modeling.
# + https://juliasilge.com/blog/sherlock-holmes-stm/.
viz_lda_gammas <-
  model_lda_gammas %>%
  mutate(topic = paste0("Topic ", topic)) %>% 
  mutate(topic = factor(topic)) %>% 
  ggplot(aes(x = gamma, fill = topic)) +
  geom_histogram(bins = 6) +
  scale_fill_func() +
  facet_wrap( ~ topic) +
  temisc::theme_te_b_facet_dx() +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL) +
  labs(title = "Distribution of Document Probabilities for Each Topic")
viz_lda_gammas
```

#### Word-To-Document Probabilities

Finally, it can be insightful to see to which document the model
assigns each individual word (as opposed to the handful topics).
In the case that the gamma probabilities are
inaccurate, identifying which words are misclassified can provide an indication
of why the model is erroneous. This can be extremely important in a real world
context.

```{r viz_lda_words_preds}
model_lda_words_preds <- model_lda %>% broom::augment(data_dtm)
model_lda_words_preds_joined <-
  model_lda_words_preds %>%
  inner_join(model_lda_gammas_preds %>% select(-gamma), by = c(".topic" = "topic"))

viz_lda_words_preds <-
  model_lda_words_preds_joined %>%
  count(document, consensus, wt = count) %>%
  group_by(document) %>%
  mutate(pct = n / sum(n)) %>%
  arrange(document, consensus) %>% 
  ggplot(aes(x = consensus, y = document, fill = pct)) +
  geom_tile() +
  scale_fill_gradient2(high = "red", label = scales::percent_format()) +
  temisc::theme_te_b() +
  theme(legend.position = "bottom") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        panel.grid = element_blank()) +
  labs(x = "Prediction", y = "Actual", fill = "% of assignments") +
  labs(title = "Prediction Accuracy of Words to Documents")
viz_lda_words_preds
```

## Conclusion

That's all I got! I think I'm going to work on developing a personal package
containing functions to re-create much of the analysis shown here.
Thanks again to David Robinson and Julia Silge for their great
[_Tidy Text Mining with R_ book](https://www.tidytextmining.com/)!
It demonstrates simple, yet powerful, techniques
that can be easily leveraged to gain meaningful insight into nearly anything you can imagine.


